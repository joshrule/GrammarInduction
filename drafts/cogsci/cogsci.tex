% Annual Cognitive Science Conference

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{graphicx}
\usepackage{floatrow}
\usepackage{amsmath,amssymb}
\usepackage[longnamesfirst]{natbib}
\usepackage{url}

\title{Representing and Learning a Large System of Number Concepts \\ with Latent Predicate Networks}

\author{
  {\large \bf Joshua Rule (rule@mit.edu)}\\
  {\large \bf Eyal Dechter (edechter@mit.edu)}\\
  {\large \bf Joshua B. Tenenbaum (jbt@mit.edu)}\\
  MIT, 46-4053, 77 Massachussetts Avenue, Cambridge, MA 02139 USA}

\begin{document}

\maketitle

\begin{abstract}
  Conventional models of exemplar or rule-based concept learning
  present the challenge of learning as one of choosing individual
  concepts from a profusion of options ({\bf EYAL: MARGOLIS
    CITATIONS}). They tend to underemphasize the fact that we learn
  many concepts as part of large systems rather than as isolated
  individuals. In such cases, the challenge of learning is not so much
  in providing stand-alone definitions, but in describing the richly
  structured relations between concepts. The natural numbers are one
  of the first such abstract conceptual systems children acquire,
  serving as a serious case study in concept representation and
  acquisition \citep{fuson1988children,galGel2005,Car2009}. Even so,
  models of natural number learning focused on single-concept
  acquisition have largely ignored two challenges related to natural
  number's status as a \emph{system} of concepts: 1) there is an
  unbounded set of exact number concepts, each with distinct semantic
  content; and 2) people can reason flexibly about any of these
  concepts (even fictitious ones like \emph{eighteen-gazillion and
    thirty-one}). To succeed, models must instead learn the structure
  of the entire infinite set of number concepts, focusing on how
  relationships between numbers support reference and generalization.
  Here, we suggest that the latent predicate network (LPN) -- a
  probabilistic context-sensitive grammar formalism -- facilitates
  tractable learning and reasoning for natural number concepts
  \citep{DecRulTen2015}. We show how the number words and their
  relations to one another can be expressed in this formalism and
  discuss a Bayesian learning algorithm for LPNs, suggesting a
  computational mechanism by which children might learn abstract
  numerical knowledge from linguistic utterances about numbers.

  \textbf{Keywords:}
  child development; concept learning; number; generalization;
  computational model; grammar induction;
\end{abstract}

\section{Introduction}

Humans seldom learn concepts in isolation. We learn about left by
comparing and contrasting it with up, down, and right, and about red
by noting its similarities and differences with green and blue. The
natural numbers (1, 2, 3, $\ldots$) are no exception: to understand a
number such as \emph{one}, we must not only ground it in terms of
concepts and percepts we already know, but we must also relate it to
other number concepts we are still in the process of acquiring. The
natural numbers are in fact particularly interesting in this respect.
Because they are infinite, there is no way to learn all the individual
concepts without learning the structure of the entire system.

A great deal of empirical work has focused on the first part of this
problem, on how initial number concepts are grounded in counting
routines and the core systems of approximate magnitude and parallel
object individuation
\citep{Car2009,dehaene2011number,feigenson2004core}. Recent studies
have also proposed computational mechanisms to explain several key
behavioral changes during early number learning
\citep{PianGoodTen2012}.

Far fewer studies have focused on the second half of the problem, on
how concepts are learned as systems and partially defined with respect
to each other. While the problems of how children link physical sets
with the counting routine and develop their first number concepts are
crucial, we direct our attention elsewhere in this paper. We focus on
this second problem, on how children might acquire knowledge of an
infinite number system, particularly for numbers they are unlikely to
ever see counted out explicitly.

Our approach shares much in common with the recent family of Rational
Rules models
\citep{goodman2008rational,T.D.Ullman:2012:1b1b6,PianGoodTen2012},
exploring concept learning through Bayesian induction of compositional
representations using sparse evidence. We agree that these points are
fundamental to understanding concept learning.

The major difference is in how our models represent concepts. In
Rational Rules models, each concept is a single stand-alone rule
supported by its own evidence. These rules are generated from a static
grammar which defines the hypothesis space. Learning is determining
which concepts (which rules) are supported by the evidence. In the
model we present here, concepts are not stand-alone rules, but
networks of possible relations generated according to a grammar. The
hypothesis space is thus not over stand-alone rules generated by a
pre-specified grammar, but over millions of possible grammars, each
defining a different network of relations. Learning is determining
which grammar, which sets of relations, are supported by the evidence.

We begin by discussing how to represent the infinite conceptual system
of natural number, and show how a particular formalism -- the
Probabilistic Range Concatenation Grammar (PRCG) -- can represent
number this way \citep{boullier2005range}. We then show how a portion
of this grammar can be learned using Bayesian inference in an LPN, a
learning framework for PRCGs \citep{DecRulTen2015}..

\section{Representing Number Knowledge}

To show how a system of concepts like number might be learned, we must
first understand what that system is and how it might be represented.
This task is non-trivial for number, and several models have been
proposed. For example, \citeauthor{hurford1975linguistic}
(\citeyear{hurford1975linguistic}) describes a single system
differentiating primitive and compound number concepts, while
\citeauthor{siegler1982development}
(\citeyear{siegler1982development}) propose a system with several
stages of development, each containing minimal internal structure.

To motivate our stance on the issue, we first describe several
challenges a representation of number must overcome. Finally, we show
how PRCGs, initially developed to explain context-sensitive syntactic
structures in natural language, can explain the conceptual structure
of number words.

\subsection{The Challenges of Number}

Relative to many other semantic fields children encounter ({\it e.g.}
color, kinship), natural numbers are highly distinctive.

First, where many semantic fields refer to relatively concrete classes
of objects or object parts, the natural numbers refer primarily to an
abstract property (cardinality) of an abstract entity (sets). Semantic
fields like colors also tend to be relatively limited in scope,
applying primarily, in this case, to the colors of objects. By
contrast, natural number is incredibly broad, applying not only to
concrete objects, but also to things like sets ({\it e.g.} three pairs
of socks), sounds, events, time periods, people and other agents, and
numbers themselves ({\it e.g.} three threes makes nine).

Second, there are infinitely many number concepts. Even given a
practically infinite number of perceivable colors ({\it e.g.} green of
sunlight through leaves), the collection of named colors is fairly
small ({\it e.g.} red, orange, yellow, green, ...). By contrast, not
only are there a practically infinite number of natural number
instances ({\it e.g.} three leaves), but a truly infinite number of
natural number concepts ({\it e.g.} three). Being infinite, the amount
of explicitly counted, perceptually-grounded evidence children receive
relative to the size of the semantic field is incredibly sparse (When
did you last see exactly 253 objects?). In order to accommodate such
an expressive conceptual system in a finite mind, the concepts
themselves must be constructed as needed in a systematic and
compositional manner. Being infinite and broadly applicable also
suggests that numbers can be learned, represented, and understood
without direct perceptual grounding.

Third, numbers do not uniquely describe cardinalities for children but
initially have distinct meanings related to sequencing, counting,
measuring, ordinality, and several non-numerical meanings ({\it e.g.}
telephone numbers) (Fuson, Richards, Briar, 1982). Even when numbers
do describe cardinalities, interest may not be so much in the
cardinality itself as in some more complex property, such as whether
it is more or less than another cardinality or how it operates
in arithmetic. Children may eventually learn about negative
and rational numbers, algebra, geometry, and myriad other mathematical
disciplines. This hugely diverse range of meanings makes it impossible
to fully describe \emph{three} without referencing \emph{two},
\emph{four}, and eventually all other numbers. The concept of
\emph{two} (or any other number) is not rightly understood as a single
object, but must also include the web of relationships in which
\emph{two} participates.

How can we hope to represent systems of concepts which are: 1)
learnable without direct perceptual grounding; 2) compositionally
constructed; and 3) relationally defined? Happily, these properties
are similar to those linguists face in studying natural language
syntax. Motivated by this similarity, we use a grammar to represent
the system of natural number concepts. Grammars can be induced
directly from a stream of utterances, are highly compositional, and
define their constituents based on their relationships to each other
rather than as discrete objects. While most work in natural language
syntax uses context-free grammars, our focus on capturing structural
relationships between concepts demands that we use a context-sensitive
grammar. We specifically use PRCGs because they are expressive and
context-sensitive while remaining relatively tractable
\citep{boullier2005range}.

\subsection{A Grammar for Number Concepts}

\begin{figure*}[t]
  \begin{centering}
    \includegraphics[width=0.9\linewidth]{grammarOfNumber/gon.pdf}
    \caption{A Range Concatenation Grammar whose strings are valid number words.}
    \label{fig:gon}
  \end{centering}
\end{figure*}

\begin{figure*}[t]
  \begin{centering}
    \includegraphics[width=\linewidth]{parseTrees/parse.pdf}
    \caption{RCG parses for \emph{Number} (Blue), \emph{Successor} (Red), and \emph{More} (Green).}
    \label{fig:parse}
  \end{centering}
\end{figure*}

Having motivated our decision to model concepts as sets of grammatical
relations, we now present the RCG-based grammar of number knowledge we
have constructed. In this initial exploration, we have captured five
key number relations: \emph{Number}, capturing the distinction between
valid and invalid number words; \emph{Succ} and \emph{Pred}, the
successor and predecessor relations, respectively; and \emph{More} and
\emph{Less}, the more-than and less-than relations, respectively.
While seemingly basic tasks, children require years to master them
\citep{FusRicBriar1982}.

Capturing these relations with an RCG is not only possible but can
be done quite compactly. Our grammar for the concepts of
\emph{Number}, \emph{Successor}, \emph{Predecessor}, \emph{Less}, and
\emph{More} covers all numbers between 0 and 1 billion, exclusive, and
requires only 216 rules. Even considering just \emph{Number},
\emph{Successor}, and \emph{Predecessor}, these 216 rules cover more
than 500 quadrillion relations. Figure \ref{fig:gon} shows a schematic
of the rules concerned with determining valid and invalid numbers,
while the rest, due to space constraints, can be found
online.\footnote{http://github.com/joshrule/GrammarInduction}

Two notes of clarification: First, our grammar never produces nor
parses full English sentences. This is a grammar for the structure of
concepts, not the structure of language. When attempting to parse
something like \emph{Succ(ninety nine, one hundred)}, we assume
another system more directly involved in language preprocesses
utterances into predicates which are then checked against the
knowledge encoded in our conceptual grammar. Second, this grammar has
not been optimized for compactness or efficiency. Implementing
\emph{More} with as the transitive closure of \emph{Successor}, for
example, would eliminate several predicates. We focused on providing a
grammar that would be correct, human-readable, and fit a
prefix-base-suffix understanding of number, as discussed below.

Intuitively, a number word like \emph{six-hundred thirty-seven} is
valid because we have six units of one hundred each and thirty-seven
remaining units of one each. That is, we have some base unit (hundred)
and we track both how many of them we have (six), and how many of the
next smallest base unit (one) we have (thirty-seven). We denote the
sum of these (six-hundred + thirty-seven) simply by concatenating the
two terms from largest to smallest base (six-hundred thirty-seven).
This structure is recursive. \emph{Nine-thousand seven-hundred
  sixteen} is created by taking nine thousands units and tacking on a
remainder, which is seven hundreds plus its remainder of sixteen ones:
\emph{nine} $\times$ \emph{thousand} $+$ (\emph{seven} $\times$
\emph{hundred} + (\emph{sixteen} $\times$ \emph{one})). Note that
there is no explicit mention of the base \emph{one} in a valid number
word - it is implied and marked by appending $\varnothing$, the empty
string, instead of \emph{one}.

Our grammar similarly uses a prefix-base-suffix system, and Figure
\ref{fig:parse} shows the concepts involved in deciding that
\emph{six-hundred thirty-seven} is a valid number word. As in our
example above, we must show that \emph{six} is a valid prefix for
\emph{hundred} and \emph{thirty-seven} is a valid suffix or remainder:

\vspace{.5em}
\emph{Number(six hundred thirty seven) $\leftarrow$}

\emph{Prefix(six, hundred), Suffix(hundred, thirty seven).}
\vspace{.5em}

\noindent\emph{Six} is a valid prefix for \emph{hundred} because it is
a number word representing a \emph{ones} number, a number between one
and nine. It would be incorrect for \emph{hundred} to have no prefix,
and it would also be incorrect to use a prefix larger than
\emph{nine}:

\vspace{.5em}
\emph{ Prefix(six, hundred) $\leftarrow$ Ones(six).}
\vspace{.5em}

\noindent\emph{Thirty-seven} is a valid suffix because it is a valid
number for a previous base, in this case $\varnothing$, the ones base:

\vspace{.5em}
\emph{Suffix(hundred, thirty seven) $\leftarrow$}

\emph{LargerBase(hundred, $\varnothing$), Number(thirty seven).}
\vspace{.5em}

\noindent and

\vspace{.5em}
\emph{LargerBase(hundred, $\varnothing$) $\leftarrow$ PrevBase(hundred, $\varnothing$).}
\vspace{.5em}

\noindent\emph{Thirty-seven} is one of these numbers because it is
merely the concatenation of a \emph{decade} word and a \emph{ones}
word:

\vspace{.5em}
\emph{ Number(thirty seven) $\leftarrow$}

\emph{Prefix(thirty seven, $\varnothing$), Suffix($\varnothing$,$\varnothing$)}.}
\vspace{.5em}        

\noindent and

\vspace{.5em}
\emph{ Prefix(thirty seven, $\varnothing$) $\leftarrow$ Decades(thirty), Ones(seven). }
\vspace{.5em}

\noindent The compositional use of simple predicates thus helps us
analyze the structure of a complex phrase like \emph{six hundred
  thirty seven} and show that while it is a valid number word,
\emph{hundred six seven thirty} is not. \emph{Successor} and
\emph{More} can similarly be encoded (Figure \ref{fig:parse}), while
\emph{Predecessor} and \emph{Less} can be encoded quite simply as
$\text{Less}(X,Y) \leftarrow \text{More}(Y,X)$ and $\text{Pred}(X,Y)
\leftarrow \text{Succ}(Y,X)$.

\section{Learning Number Knowledge}

How might children learn the knowledge about number that is captured
in the representation presented above? In this section, we present a
computational model of learning over RCGs and take a first step
towards evaluating this model against the learning trajectories and
patterns of error reported in the literature on counting. 

Because the learning problem is computationally challenging and
because much of the focus in the literature on exact number knowledge
has been on counting, we restrict our experiments here to the
successor relation, and, in particular, to learning how to count from
one to a hundred.

Learning to count up to a hundred does not come easily. Studies of
counting in children between three and six years of age, suggests that
although some children are able to count up to a hundred shortly
before entering kindergarten, many have trouble with the task at least
as late as first
grade~\cite{FusRicBriar1982,miller1987counting}. These studies suggest
that children do not learn to count to a hundred by memorizing the
sequence of numbers between on and a hundred. As might be expected of
a system that will eventually be able to count to arbitrarily large
numbers, learning to count to a hundred involves discovering the
pattern that successive number words follow. 

We will model this process of pattern discovery as probabilistic
inference over RCGs and show that the generalization phenomena
described in these studies is captured by this model. 

\subsection{Latent Predicate Networks}

\begin{figure}[t]
  \begin{centering}
    \includegraphics[width=0.9\linewidth]{figures/lpn}
    \caption{A schematic LPN for number knowledge learning.}
  \end{centering}
\end{figure}

Latent Predicate Networks (LPNs) are PRCGs with a large number of
rules connected in a layered fashion. An schematic of the LPN used in
the following learning experiments and simulations is show in
Figure~\ref{fig:lpn}. There are three types of predicates in LPNs. The
\emph{observed} predicates are relations that are directly present in
the data (e.g. the successor predicate Succ is observed if the data is
a collection of successor pairs). The observed predicates are defined
in terms of layers of \emph{latent} predicates. These are relations
that are not directly observable in the data and whose meanings are
determined through learning. For example, the Decade predicate, which
is true for ``ten'', ``twenty'', etc., might correspond to one of the
latent predicates after learning on successor pair data. The lowest
layer of latent predicates is defined in terms of a collection of
\emph{lexicon} predicates, each of which is a unary predicate that is
true of the atomic units (the words) of the system.

Given a collection of observations, our model assumes that there is a
setting of the parameters of the LPN that gave rise to those
observations. The model's goal is to infer what those parameters
are. This inference is conducting via hierarchical Bayesian
inference~(CITATION): the model assumes that there is a prior
distribution over the parameters of the LPN and that given those
parameters the LPN induces a distribution over observations. Using
Bayes' rule, the model infers a distribution over parameter values
that balances the fit of the observations against the prior. We use a
sparsity inducing prior distribution which formalizes the intuition
that latent predicates and rules should be shared in order to learn
grammars that can generalize beyond the observed data.

Exact inference in probabilistic grammars is computationally
intractable. We use the common ``variational'' approximation
implemented using Variational Bayes EM algorithm~(CITATION), which has
been widely used in approximate inference of probabilsitic context
free grammars~(CITATION). All the computations in this paper were done
using the PRISM probabilistic logic programming language~(CITATION).

All the simulations below were run on an LPN with three layers of five
predicates each. The learning algorithm was run for a single
iteration, a concentration parameter $alpha=0.1$, and a convergence
criterion of $\epsilon=1e-4$.

\subsection{Modeling the acquisition and elaboration of the count sequence}

\citet{FusRicBriar1982} describe qualitative phenomena of
count sequence acquisition and elaboration based on several surveys in
which they asked American children between three and five years of age
to count (either while counting a collection of objects or just
reciting the count world sequence). The learning trajectories and
error patterns they describe have inspired computation modeling
efforts using connectionist networks; for example,
\citet{ma1989modeling} use an associative network to model the errors
that young children typically make when learning the count sequence up
to twenty or thirty. But, to our knowledge, such models have not been
used to study how children acquire the count sequence beyond twenty.


\begin{figure*}[t]
\includegraphics[width=0.9\linewidth]{figures/modelboxplot}
\caption{Data from~\citet{FusRicBriar1982} on children's ability to
  recite the count list. The x-axis shows the highest number correctly
  reached when children were asked to count starting at ``one.'' Boxes
  correspond to the standard deviation, central bands to the means,
  and the whiskers to the range.
\label{fig:fuson_count_data}}
\end{figure*}

Figure~\ref{fig:fusonTable} shows the highest number correctly reached
by children in various age ranges~\cite{FusRicBriar1982}. The authors
hypothesize that the large jump in range between the young three year
olds and the older four year olds and year olds is due to the older
children partially solving what they term ``the decade problem.'' --
i.e. recognizing both that there is a repetitive pattern of that
repeats across decades beyond twenty and learning that there is a
particular sequence to the decade words.

We asked whether our model goes through a similar transition. To
simulate learning, we generated data sets consisting of successor
pairs between one and a hundred, with the number of examples of each
pair $Succ(i+1, i)$ following the power law $N=\frac{K}{i}$, where $K$
determines the overall size of the data set. The explore the effect of
evidence quantity, and to simulate the effect that overall quanitity
of evidence has on a child's acquisition of the count list, we
generated data sets for $K=10, 100, 1000, 10000$, which we denote
stages 1-4. The resulting histograms of data are shown in
Figure~\ref{fig:counting_grid}a-d (the y-axes are logarithmically
scaled).

For each of these data sets we ran our learning algorithm ten times,
generating ten simulated children at each stage (the simualations
differ due to different random parameter intiailizations). In
Figure~\ref{fig:counting_grid}a-d, each line corresponds to one of the
simulated children and shows the probability that the child will
correctly count to the corresponding number on the x-axis. To generate
this counting simulations, we asked the model for the distribution of
successors for a given number and used a simple soft max decision
procedure to determine the probability of the simulated child
reporting each word. Specifically, if the $p_a(x)$ is the probability
that $x$ is the successor of $x$, then the probability that $x$ is
chosen as the successor of $a$ is proportional to $p_z(x)^2$.

The simulations in stage 1 are variable in their performances, with
some of simulated children unable to count further than the first few
words and couple having a relatively high chance of reaching
``twenty.'' While there is a sharp cutoff at ``twenty'' for this
group, which is exceeded by the majority of simulated children in
stage 2. The sharp drops in performance at the ``twenty,'' ``thirty''
and ``forty'' in stage 2, and the horizontal lines between them,
indicate that here the simulation has learned the within-decade
structure of the count list, but is uncertain about the transitions
between decades. In stages 3 and 4, nearly all simulated children
master the numbers up to ``thirty'' but are unable to transition from
``thirty-nine'' to ``forty.'' And only in stage 4, do we see any
children making the transition from this state of knowledge to one in
which it can reach ``ninety nine.'' 

The simulations in these first four stages suggest that with even with
large increase in the quantity of data, the 1-to-29 counter mode is
likely to trap our model. We hypothesized that this is due to a lack
of evidence at the decade transitions. Mastering the decade
transitions requires both learning that there is a special rule for
the successor of numbers ending in ``nine,'' and learning the order of
decade words. This adds considerable complexity to the grammar, and
our simulations favor a more parsimonious explanation of the heavily
weighted smaller numbers. Children, however, do not learn to count to
a hundred by unsupervised exposure to naturally occuring count words;
they are actively taught to do so. Although we know of no study of the
pedagogical language used in teaching children to count, increased
emphasis on difficult regions of the count list is a likely
consequence.

To confirm that increased emphasis on decade transitions can
facilitate the transition to mastering counting up to a hundred, we
created two additional data sets, stages 3' and 4', that contain the
same data as stages 3 and 4, respectively, but have an additional
$\%10$ of the data evenly distributed across the decade
transitions (twenty nine, thirty; thirty nine, forty; ...,
eighty-nine, ninety). The simulated data for these stages is shown in
Figure~\ref{fig:count_grid2}e-f}.

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{figures/counting_grid2}
  \floatbox[{\capbeside\thisfloatsetup{capbesideposition={left,center},capbesidewidth=.75\textwidth}}]{figure}[\FBwidth]
           {\caption{Our model's performance correctly reciting the
               count sequence. Each line corresponds to a single run
               of the learning algorithm given the distribution of
               data in the histogram directly above it. For each
               number, $N$, along the x-axis, the y-axis corresponds
               to the probability that the model correctly counts from
               one up to $N$. The y-axes on the data histograms are
               shown on a logarthmic scale. The simulations in a) were
               trained on the least data and those in b-d) on
               increasing quantities of data. e) \& f) were trained on
               the same data as c) \& d), respectively, with extra
               emphasis placed on the decade
               transitions.}\label{fig:counting_grid}}
           {\includegraphics[width=.2\textwidth]{figures/legend.pdf}}
\end{figure*}

\begin{figure}[t]
\includegraphics[width=0.9\linewidth]{figures/after29}
\caption{The distribution over successors of ``twenty nine'' given learning on the evidence corresponding to Figure~\ref{fig:counting_grid}b. \label{fig:after29}}
\end{figure}


\begin{figure}[t]
\includegraphics[width=0.9\linewidth]{figures/inventedWords}
\caption{The distribution over invented words for simulated child
  learner at stage
  3 (see Figure~\ref{fig:counting_grid}c). \label{fig:inventedWords}}
\end{figure}


\section{Discussion}

%% first half: Eyal

%% Either way, as these models claim to cover more well-trodden
%% territory, they should also become correspondingly more quantitative
%% in their predictions. The ability to generalize as well as the
%% qualitative similarities we show here between LPNs and children's
%% overgeneralizations are intriguing, but 

%% second half of discussion

While the conceptual grammar contains a great deal of knowledge about
\emph{fifty-two}, there is no single object that in any sense
completely defines \emph{fifty-two}. The definition is heavily
influenced by the entire collection of relationships which hold for
the token \emph{fifty-two}. In a similar vein, the names given to
specific predicates in our grammar are arbitrary. \emph{Number} could
just as easily be called \emph{Yulmpa} or \emph{Snarg}. What is
important is not a predicate's name, but the relations it enters into
with other predicates and thus the argument strings for which it
holds. The same is true for individual words like \emph{hundred} or
\emph{nine}: while any name could be used, what cannot be changed is
the way each predicate combines or contrasts a given token with other
tokens in the grammar.

As mentioned in the introduction, the LPN model of concept learning
shares many similarities with the Rational Rules family of models
\citep{goodman2008rational,T.D.Ullman:2012:1b1b6,PianGoodTen2012},
particularly our focus on learning concepts from sparse evidence using
Bayesian induction over compositional representations. We differ in
how we use grammars. Rational Rules models use a grammar to generate
sentences, each of which represents a single concept to be learned.
LPNs, by contrast, learn a grammar which generates relations, each of
which describes a particular interaction between a set of concepts.

For example, the Rational Rules-based model of Cardinal Principle (CP)
learning given in \citep{PianGoodTen2012} is focused on finding a
specific sentence which captures the CP, in this case a program in the
simply-typed lambda calculus. An LPN-based approach would be to learn
a complete grammar over many number concepts, the relationships
between some of which happen to correctly encode the CP.

Note, however, that this paper focuses on a different psychological
problem than Piantadosi and colleagues (\citeyear{PianGoodTen2012}) in
that we are not proposing a model of how children acquire or ground
their initial number concepts or the CP. Neither our hand-crafted nor
our learned \emph{Number} predicates contain any link between set
cardinalities and number words, nor does \emph{More} connect in any
way to approximate magnitude. We intend to explore these connections
in future work but focus here on the fact that to know what the number
words mean is to know how they interact with each other as well as
concepts you already have, including (potentially innate) pre-existing
conceptions of \emph{More} or \emph{Less} used to differentiate sets,
or the version of \emph{Successor} used to memorize songs and
routines.

That said, we see no fundamental incompatibility between the model
presented here and extensions to include approximate magnitude, object
tracking, set manipulation, more complex morphology ({\it e.g.} the
meaning of \emph{-illion} or \emph{-teen}), or different counting
strategies ({\it e.g.} as used in Turkish, French, or Mandarin) as
would be needed for a more comprehensive model of number learning. Far
from it, we see our work here as a first demonstration of LPN's
suitability for capturing a broad range of concepts in number and
other semantic domains including space, kinship, and natural kinds.
Whether these more general models are best approached by working
strictly within the LPN formalism or by using it as one module within
a more complex framework is an open question. Certainly, the human
mind is more powerful than an RCG and is at least Turing-complete.
RCGs provide a tractable way, however, to explore a restricted
subclass of problems. The strategies and solutions we discover here
are also available in Turing-complete systems, and are in fact
implemented in one (PRISM Prolog), so our findings easily generalize
to more expressive grammars.

More broadly, we see this paper as growing out of the hypothesis that
much of human learning, including the explosion of knowledge that
occurs during development, can be explained as induction in a formal
language. This vision of the \emph{child-as-hacker} draws on and
extends the notion of the \emph{child-as-scientist} \citep{gopnik1996scientist}; not only are
children forming theories about the world, but they are simultaneously
developing the very conceptual language they use to formulate those
theories.

\section{Acknowledgments}

The authors benefited significantly from conversations with Timothy
O'Donnell and Leon Bergen. This material is based upon work supported
by the Center for Minds, Brains and Machines (CBMM), funded by NSF STC
award CCF-1231216, an NSF Graduate Research Fellowship, and the Eugene
Stark Graduate Fellowship.


\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}
\bibliography{cogsci}

\end{document}
