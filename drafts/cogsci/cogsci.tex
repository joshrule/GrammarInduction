% Annual Cognitive Science Conference

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage[natbibapa]{apacite}
\usepackage{graphicx}
\usepackage{floatrow}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage[longnamesfirst]{natbib}
\usepackage{url}

\renewcommand{\bibfont}{\small}
\renewcommand\bibsection{%
  \section*{\refname\markboth{\MakeUppercase{\refname}}{\MakeUppercase{\refname}}}%
  \addcontentsline{toc}{section}{\refname}%
}

\title{Representing and Learning a Large System of Number Concepts\\with Latent Predicate Networks}

\author{
  {\large \bf Joshua Rule, Eyal Dechter, Joshua B. Tenenbaum: $\{$rule, edechter, jbt$\}$ @ mit.edu}\\
  MIT, 46-4053, 77 Massachussetts Avenue, Cambridge, MA 02139 USA}

\begin{document}

\maketitle

\begin{abstract}
  Conventional models of exemplar or rule-based concept learning tend
  to focus on the acquisition of one concept at a time. They often
  underemphasize the fact that we learn many concepts as part of large
  systems rather than as isolated individuals. In such cases, the
  challenge of learning is not so much in providing stand-alone
  definitions, but in describing the richly structured relations
  between concepts. The natural numbers are one of the first such
  abstract conceptual systems children acquire, serving as a serious
  case study in concept representation and acquisition
  \citep{fuson1988children,galGel2005,Car2009}. Even so, models of
  natural number learning focused on single-concept acquisition have
  largely ignored two challenges related to natural number's status as
  a \emph{system} of concepts: 1) there is an unbounded set of exact
  number concepts, each with distinct semantic content; and 2) people
  can reason flexibly about any of these concepts (even fictitious
  ones like \emph{eighteen-gazillion and thirty-one}). To succeed,
  models must instead learn the structure of the entire infinite set
  of number concepts, focusing on how relationships between numbers
  support reference and generalization. Here, we suggest that the
  latent predicate network (LPN) -- a probabilistic context-sensitive
  grammar formalism -- facilitates tractable learning and reasoning
  for natural number concepts \citep{DecRulTen2015}. We show how a
  subset of the number concepts and their relations to one another can
  be expressed in this formalism and discuss a Bayesian learning
  algorithm for LPNs, suggesting a computational mechanism by which
  children might learn abstract numerical knowledge from linguistic
  utterances about numbers.

  \textbf{Keywords:}
  child development; concept learning; number; generalization;
  computational model; grammar induction;
\end{abstract}

\section{Introduction}

Humans seldom learn concepts in isolation. We learn about \emph{left}
by comparing and contrasting it with \emph{up}, \emph{down}, and
\emph{right}, and about \emph{red} by noting its similarities and
differences with \emph{green} and \emph{blue}. The natural numbers (1,
2, 3, $\ldots$) are no exception: to understand a number such as
\emph{one}, we must not only ground it in terms of concepts and
percepts we already know, but we must also relate it to other number
concepts we are still in the process of acquiring. The natural numbers
are particularly interesting in this respect. Because they are
infinite, there is no way to learn all the individual concepts without
learning a compositional structure for the system.

A great deal of empirical work has focused on the first part of this
problem, on how initial number concepts are grounded in counting
routines and the core systems of approximate magnitude and parallel
object individuation
\citep{Car2009,dehaene2011number,feigenson2004core}. Recent studies
have also proposed computational mechanisms to explain several key
behavioral changes during early number learning
\citep{PianGoodTen2012}.

Far fewer studies have focused on the second half of the problem, on
how numbers are learned as a system and partially defined with respect
to each other. While the problems of how children link physical sets
with the counting routine and develop their first number concepts are
crucial, we direct our attention elsewhere in this paper. We focus on
this second problem, on how children might acquire knowledge of a
compositional number system, particularly for numbers they hear
discussed but are unlikely to ever see counted out explicitly.

We ground our learning proposal in a new framework for representing
number as a conceptual system, which on its own has presented a
non-trivial challenge met in different ways by linguists and
developmentalists. For example, \citeauthor{hurford1975linguistic}
(\citeyear{hurford1975linguistic}) proposed a single system
differentiating primitive and compound number concepts, while
\citeauthor{siegler1982development}
(\citeyear{siegler1982development}) proposed a system with several
stages of development, each containing minimal internal structure.

Our approach to representation and learning is in part inspired by,
and shares much in common with, the recent family of Rational Rules
models
\citep{goodman2008rational,T.D.Ullman:2012:1b1b6,PianGoodTen2012},
exploring concept learning through Bayesian induction of compositional
representations using sparse evidence. We agree that these points are
fundamental to understanding concept learning.

The major difference is in how our models represent concepts. In
Rational Rules models, each concept is a single stand-alone rule
supported by its own evidence. These rules are generated from a static
grammar which defines the hypothesis space. Learning is determining
which concepts (which rules) are supported by the evidence. In the
model we present here, concepts are not stand-alone rules, but
networks of possible relations generated according to a grammar. The
hypothesis space is thus not over stand-alone rules generated by a
pre-specified grammar, but over millions of possible grammars, each
defining a different network of relations. Learning is determining
which grammar, which sets of relations, are supported by the evidence.

We begin by discussing how to represent the infinite conceptual system
of natural number, and show how a particular formalism -- the
Probabilistic Range Concatenation Grammar (PRCG) -- can represent a
subset of the number concepts this way \citep{boullier2005range}. We
then show how a portion of this grammar can be learned using Bayesian
inference in an LPN, a learning framework for PRCGs
\citep{DecRulTen2015}.

\section{A Grammar Representing Number Knowledge}

Three challenges make learning systems of number concepts particularly
difficult and interesting. First, very few of our number concepts are
perceptually grounded. When, for example, did you last count exactly
254 objects? The problem only intensifies as we begin applying numbers
to events, time periods, sets of objects, and eventually even other
numbers. Second, the fact that there are infinitely many number
concepts means that, much like in natural language sentences, the
meanings of the numbers are compositional. The meaning of
\emph{four-hundred fifty-two}, for example, depends on but goes
significantly beyond the meanings of \emph{four} and \emph{hundred}.
Third, to understand numbers is also to understand the relations in
which numbers participate. We are often interested in a number not so
much for its cardinality as for some more complex property, such as
whether it is more or less than another number or how it changes
through addition or division. This hugely diverse range of uses makes
it impossible to fully describe \emph{three} without referencing
\emph{two}, \emph{four}, and eventually all other numbers.

How can we hope to represent systems of concepts which are: 1)
learnable without direct perceptual grounding; 2) compositionally
constructed; and 3) relationally defined? Happily, these properties
are similar to those linguists face in studying natural language
syntax. Grammars can be induced directly from a stream of utterances,
are highly compositional, and define their constituents based on their
relationships to each other rather than as discrete objects.

\begin{figure*}[t]
  \begin{centering}
    \includegraphics[width=\linewidth]{grammarOfNumber/gon.pdf}
    \caption{An RCG whose strings are valid number words. Numbered rules correspond to Figure \ref{fig:parse}.}
    \label{fig:gon}
  \end{centering}
\end{figure*}

\begin{figure*}[t]
  \begin{centering}
    \includegraphics[width=\linewidth]{parseTrees/parse.pdf}
    \caption{RCG parses for \emph{Number} (Blue) and \emph{Successor} (Red).}
    \label{fig:parse}
  \end{centering}
\end{figure*}

Motivated by this insight, we now present a grammar of number
knowledge we have constructed to capture five key number relations
learned during childhood and carried into adulthood: \emph{Number},
capturing the distinction between valid and invalid number words;
\emph{Succ} and \emph{Pred}, the successor and predecessor relations,
respectively; and \emph{More} and \emph{Less}, the more-than and
less-than relations, respectively. While seemingly basic tasks,
children require years to master them \citep{FusRicBriar1982}. Whereas
most work in natural language syntax uses context-free grammars, our
focus on capturing structural relationships between concepts demands
that we use a context-sensitive grammar. We specifically use PRCGs
because they are expressive and context-sensitive while remaining
relatively tractable \citep{boullier2005range}.

Capturing these relations with an RCG is not only possible but can be
done quite compactly. Our grammar for the concepts of \emph{Number},
\emph{Successor}, \emph{Predecessor}, \emph{Less}, and \emph{More}
covers all numbers between 0 and 1 quadrillion, exclusive, and
requires only 218 rules. Even considering just \emph{Number},
\emph{Successor}, and \emph{Predecessor}, these 218 rules cover more
than $10^{24}$ true relations. Figure \ref{fig:gon} shows a schematic of
the rules concerned with determining valid and invalid numbers, while
the rest, due to space constraints, can be found online
(http://github.com/joshrule/GrammarInduction).

Three notes of clarification: First, our grammar never produces nor
parses full English sentences. We model the structure of concepts, not
the structure of language. When attempting to parse something like
\emph{Succ(ninety nine, one hundred)}, we assume another system more
directly involved in language preprocesses utterances into predicates
which are then checked against the knowledge encoded in our conceptual
grammar. Second, this grammar has not been optimized for compactness
or efficiency. Implementing \emph{More} as the transitive closure of
\emph{Successor}, for example, would eliminate several predicates. We
focused on providing a grammar that would be correct, human-readable,
and fit a prefix-base-suffix understanding of number, as discussed
below. We do not claim this is the actual conceptual framework used by
children or adults but rather that it captures equivalent information.
Third, while the natural numbers form an infinite set, many of these
numbers do not have convenient names. Our choice to examine what can
be learned from the number names used in linguistic utterances means
we examine only a finite subset of the natural numbers.

Intuitively, a number word like \emph{six-hundred thirty-seven} is
valid because we have six units of one hundred each and thirty-seven
remaining units of one each. That is, we have some base unit (hundred)
and we track both how many of them we have (six), and how many of the
next smallest base unit (one) we have (thirty-seven). We denote the
sum of these (six-hundred + thirty-seven) simply by concatenating the
two terms from largest to smallest base (six-hundred thirty-seven).
This structure is recursive. \emph{Nine-thousand seven-hundred
  sixteen} is created by taking nine thousands units and tacking on a
remainder, which is seven hundreds plus its remainder of sixteen ones:
\emph{nine} $\times$ \emph{thousand} $+$ (\emph{seven} $\times$
\emph{hundred} + (\emph{sixteen} $\times$ \emph{one})). Note that
there is no explicit mention of the base \emph{one} in a valid number
word - it is implied and marked by appending $\varnothing$, the empty
string, instead of \emph{one}.

Our grammar similarly uses a prefix-base-suffix system, and Figure
\ref{fig:parse} shows the concepts involved in deciding that
\emph{six-hundred thirty-seven} is a valid number word. As in our
example above, we must show that \emph{six} is a valid prefix for
\emph{hundred} and \emph{thirty-seven} is a valid suffix or remainder:
\setlength{\jot}{0pt}
\begin{align}
  & \textit{Number}(\textit{six hundred thirty seven}) \leftarrow\\
  & \textit{Prefix}(\textit{six}, \textit{hundred}), \textit{Suffix}(\textit{hundred}, \textit{thirty seven}).\notag
\end{align}

\noindent\emph{Six} is a valid prefix for \emph{hundred} because it is
a number word representing a \emph{ones} number, a number between one
and nine. It would be incorrect for \emph{hundred} to have no prefix,
and it would also be incorrect to use a prefix larger than
\emph{nine}:
\begin{align}
  & \textit{Prefix}(\textit{six}, \textit{hundred}) \leftarrow \textit{Ones}(\textit{six}).
\end{align}

\noindent\emph{Thirty-seven} is a valid suffix because it is a valid
number for a previous base, in this case $\varnothing$, the ones base:
\begin{align}
  & \textit{Suffix}(\textit{hundred}, \textit{thirty seven}) \leftarrow\\
  & \hspace{0.5cm}  \textit{LargerBase}(\textit{hundred}, \varnothing), \textit{Number}(\textit{thirty seven}). \notag\\
  \notag\\
 &  \textit{LargerBase}(\textit{hundred}, \varnothing) \leftarrow \textit{PrevBase}(\textit{hundred}, \varnothing).
\end{align}

\noindent\emph{Thirty-seven} is one of these numbers because it is
merely the concatenation of a \emph{decade} word and a \emph{ones}
word:
\begin{align}
  & \textit{Number}(\textit{thirty seven}) \leftarrow\\
  & \hspace{0.5cm} \textit{Prefix}(\textit{thirty seven}, \varnothing), \textit{Suffix}(\varnothing,\varnothing).\notag\\
  \notag\\
  & \textit{Prefix}(\textit{thirty seven}, \varnothing) \leftarrow\\
  & \hspace{0.5cm} \textit{Decades}(\textit{thirty}), \textit{Ones}(\textit{seven}).\notag
\end{align}


\noindent The compositional use of simple predicates thus helps us
analyze the structure of a complex phrase like \emph{six hundred
  thirty seven} and show that while it is a valid number word,
\emph{hundred six seven thirty} is not. \emph{Successor} can similarly
be encoded (Figure \ref{fig:parse}) as can \emph{More} (not shown),
while \emph{Predecessor} and \emph{Less} can be encoded quite simply
as $\text{Less}(X,Y) \leftarrow \text{More}(Y,X)$ and
$\text{Pred}(X,Y) \leftarrow \text{Succ}(Y,X)$.

\section{Learning Number Knowledge}

How might children learn the number knowledge captured in the
representation above? In this section, we present a computational
model of learning PRCGs and take a first step toward evaluating this
model against the learning trajectories and patterns of error reported
in the literature on counting.

To match the literature's focus on counting, we restrict our
experiments here to the successor relation, and, in particular, to
learning to count from one to one-hundred. Several studies track
children's learning trajectories and patterns of errors when acquiring
the count sequence~\citep{FusRicBriar1982,miller1987counting}, making
count sequence learning an interesting domain for evaluating our model
of learning PRCGs against empirical data.

\subsubsection{Latent Predicate Networks}

Latent Predicate Networks (LPNs) are PRCGs with three types of
predicates connected in a layered fashion. \emph{Observed} predicates
are relations directly present in the data ( e.g. \emph{Successor} is
observed if the data includes \emph{Successor} pairs). Observed
predicates are defined in terms of layers of \emph{latent} predicates.
These relations are not directly observable in the data and their
meanings are determined through learning. For example, the
\emph{Decade} predicate, which is true for ``ten'', ``twenty'', etc.,
might correspond to one of the latent predicates after the model is
trained on pairs of successive number words. Each layer of latent
predicates is defined in terms of the latent predicate layer beneath
it, and the lowest layer of latent predicates is defined in terms of a
collection of \emph{lexicon} predicates, each of which is a unary
predicate that is true of the atomic units (the words) of the system.
The rules of the LPN consist of all definitions possible within the
network architecture (for details see~\cite{DecRulTen2015}). The
parameters of the network are the probabilities of the rules.

Our model learns a distribution over the parameters of the LPN given
the available data using hierarchical Bayesian inference: the model
assumes that there is a prior distribution over the parameters of the
LPN and, using Bayes' rule, infers a distribution over parameter
values that balances the fit of the observations against the prior. We
use a sparsity-inducing prior to formalize the intuition that latent
predicates and rules should be shared in order to learn grammars that
can generalize beyond the observed data.

Since exact inference in probabilistic grammars is computationally
intractable, our model is simulated using the Variational Bayes EM
approximate inference algorithm as implemented in the PRISM
programming language~\citep{sato2008variational}.

All the simulations below were run on an LPN with three layers of five
predicates each. The learning algorithm was run for a single
iteration with a concentration parameter of $\alpha=0.1$, and a convergence
criterion of $\epsilon=1e-4$. We will refer to each separate
simulation below as a \emph{simulated child}.

\subsubsection{Acquiring the count sequence}

\cite{FusRicBriar1982} describe qualitative phenomena of count
sequence acquisition and elaboration based on several surveys in which
they asked American children between three and five years of age to
count (either while counting a collection of objects or just reciting
the count word sequence). The learning trajectories and error patterns
they describe have inspired computational modeling efforts using
connectionist networks; for example, \citet{ma1989modeling} use an
associative network to model the errors that young children typically
make when learning the count sequence up to twenty or thirty. But, to
our knowledge, such models have not been used to study how children
acquire the count sequence beyond thirty.

Figure~\ref{fig:fuson_model_comparison}a shows the highest number
correctly reached by children of various ages
in~\citeauthor{FusRicBriar1982} The authors hypothesize that the large
jump in range between the young three-year-olds and the older
four-year-olds and five-year-olds is due to the older children
partially solving what they term ``the decade problem.'' --  i.e.
recognizing both that there is a pattern that repeats across decades
greater than twenty and that there is a particular sequence to the
decade words.

We asked whether our model goes through a similar transition. To
simulate learning, we generated data sets consisting of successor
pairs between one and one-hundred, with the number of examples $N$ of each
pair $Succ(i+1, i)$ following the power law $N=\frac{K}{i}$, where $K$
determines the overall size of the data set. To explore the effect of
evidence quantity, and to simulate the effect that overall quantity
of evidence has on a child's acquisition of the count list, we
generated data sets for $K=10, 100, 1000, 10000$, which we denote
stages 1-4, respectively. The resulting histograms of data are shown in
Figure~\ref{fig:counting_grid}a-d (the y-axes are logarithmically
scaled).

For each of these data sets we ran our learning algorithm ten times,
generating ten simulated children at each stage (the simulations
differ due to different random parameter initializations). In
Figure~\ref{fig:counting_grid}a-d, each line corresponds to one of the
simulated children and shows the probability that the child will
correctly count to the corresponding number on the x-axis. To generate
this data, we asked the model for the distribution of successors for a
given number and used a simple soft max decision procedure to
determine the probability of the simulated child reporting each word.
Specifically, if the simulated child believes $x$ follows $a$
with probability $p_a(x)$, then it says $x$ after $a$ with probability
proportional to $p_a(x)^2 $.

\begin{figure}[t]
  \includegraphics[width=0.9\linewidth]{figures/modelboxplot.pdf}
  \caption{Our model compared with children's counting data a) Data
    from~\citet{FusRicBriar1982}. The x-axis shows the highest number
    correctly reached when children were asked to count starting at
    ``one.'' Boxes correspond to the standard deviation, central bands
    to the means, and whiskers to the range. b) Model performance,
    averaged over ten runs at four stages of increasing data
    quantity.}\label{fig:fuson_model_comparison}
\end{figure}


The stage 1 simulations are variable in performance, with some of
simulated children unable to count further than the first few words
and a few having a relatively high chance of reaching ``twenty.'' The
sharp drops in performance at ``twenty,'' ``thirty'' and ``forty'' in
stage 2, and the horizontal lines between them, indicate that here the
simulation has learned the within-decade structure of the count list
but is uncertain about the transitions between decades. In stages 3
and 4, nearly all simulated children master the numbers up to ``twenty
nine'' but are unable to transition from ``twenty nine'' to ``thirty.''
Only in stage 4 do we see any children making the transition from this
state of knowledge to one in which they can reach ``ninety nine.''

The simulations in these first four stages suggest that even with
large increases in the quantity of data, our model is unlikely to
progress beyond ``twenty nine''. We hypothesized that this is due to a
lack of evidence for the decade transitions. Mastering the decade
transitions requires both learning that there is a special rule for
the successor of numbers ending in ``nine,'' and learning the order of
decade words. This adds considerable complexity to the grammar, and
our simulations favor a more parsimonious explanation of the heavily
weighted smaller numbers. Children, however, do not learn to count to
a hundred by unsupervised exposure to naturally occurring count words;
they are actively taught to do so. Although we know of no study of the
pedagogical language used in teaching children to count, some
kindergarten teaching blogs (e.g.
http://www.heidisongs.com/blog/2012/05/teaching-kids-to-count-to-100.html)
mention emphasizing decade transitions as useful in helping struggling
students to learn the count sequence.

To confirm that increased emphasis on decade transitions can
facilitate the transition to mastering counting up to a hundred, we
created two additional data sets, stages $3^*$ and $4^*$, that contain the
same data as stages 3 and 4, respectively, but have an additional
$10\%$ of the data evenly distributed across the decade transitions
(twenty nine, thirty; thirty nine, forty; ..., eighty nine,
ninety). The simulated data for these stages is shown in
Figure~\ref{fig:counting_grid}e-f). In both simulations, we observe a
sharp increase in the number of simulated children who transition to
counting to a hundred (from 0 to 4 children in stage $3^*$, and 2 to
6 children in stage $4^*$).

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{figures/counting_grid2}
  \caption{Our model's performance correctly reciting the count
    sequence. Each colored curve corresponds to a single run of the learning
    algorithm given the distribution of data in the histogram directly
    above it. For each number, $N$, along the x-axis, the y-axis
    corresponds to the probability that the model correctly counts
    from one up to $N$. The y-axes on the data histograms are shown on
    a logarithmic scale. The stages refer to the distributions of data
    available to the model (see text for details).
  }\label{fig:counting_grid}
\end{figure*}


Figure~\ref{fig:fuson_model_comparison}b summarizes the simulation
data for stages 1,2,$3^*$ and $4^*$ for comparison against the
\citeauthor{FusRicBriar1982} data in
Figure~\ref{fig:fuson_model_comparison}a. For each stage and each
simulated child, we computed the probability that the highest number
reached by counting, starting from ``one,'' would be $x$ for $x=1
\dots 98$. We averaged these values across simulated children within a
stage and used the resulting densities to calculate the means,
standard deviations, and 10th and 90th percentiles for each stage
(these percentiles were chosen to be comparable with the empirical
ranges described by \citeauthor{FusRicBriar1982}).

In addition to examining the learning trajectories of our model, we
also examined the kinds of mistakes that it makes. One interesting
pattern of mistakes that young English speaking children make when
reciting the count sequence is that they invent number
words. \citeauthor{FusRicBriar1982} report that children invent such
words both by combining morphological components of number words (such
as ``fiveteen'' and ``eleventy'') and by combining decade words with
incorrect digit-place words (such as ``twenty-eleven'' and
``twenty-twenty''). In particular, they report that appending
teen words to decade words is most common, creating sequences
like ``twenty-ten'', ``twenty-eleven'', ``twenty-twelve'', etc.
In Figure~\ref{fig:inventedWordComparison}a we show the most common invented
words that \citeauthor{FusRicBriar1982} report and the mean number of
times a child used the word. 

Since we do not model in this work the way in which the number word
morphemes are composed to construct number words, our model cannot
account for morphologically-based errors. We asked, however, to what
extent it can model the other invented word errors that
\citeauthor{FusRicBriar1982} report; the most common invented words
are shown in Figure~\ref{fig:inventedWordComparison}a). To compare
these data to our model, we asked a stage 2 simulated child for the
top ten non-number words that could appear as successor to a number
word or non-number word (because this was a computationally expensive
procedure, we restrict our analysis here to a single randomly selected
simulation). The marginal probabilities of those non-number words is
shown in Figure~\ref{fig:inventedWordComparison}b.

\begin{figure}[t]
\includegraphics[width=0.9\linewidth]{figures/inventedWordComparison3}
\caption{Top ten invented number words in children's counting. a) Data
  from \citeauthor{FusRicBriar1982} b) A simulated child at stage~2.
   \label{fig:inventedWordComparison}}
\end{figure}

\section{Discussion}

In this work, we have shown how a portion of the infinite set of exact
number concepts (natural numbers with common names) and the relations
among them can be represented using probabilistic context-sensitive
grammars. We have also given a model for how children might learn such
representations based on hierarchical Bayesian inference. Our
simulations suggest this model captures several behavioral phenomena
children exhibit learning the count sequence -- a critical and
difficult prerequisite to adult-like numerical knowledge.

An interesting aspect of this process is the seemingly sudden
transition from counting only through the first few decades to
counting all the way to a hundred. Our model explains this transition
as an inductive leap: for small amounts of data, learning is slow and
incremental -- adding a decade at a time -- because the increased
complexity of the conceptual knowledge is large compared to the gains
in explanatory power. Eventually, however, enough evidence accumulates
to warrant a more complex and more general grammar, resulting
in a kind of phase transition between states of knowledge.

In many ways this phenomenon is analogous to the Cardinal Principle
(CP) transition, in which younger children learning the relationship
between small numbers and set sizes make slow and incremental progress
when learning to count out sets matching the first three or four
number words but then suddenly expand their ability to every other
memorized number word. The theory that this rapid transition is due to
what Carey (\citeyear{Car2009}) refers to as \emph{Quinian
  bootstrapping} has been formalized by Piantadosi et
al.~(\citeyear{PianGoodTen2012}) as probabilistic inference over a
space of recursive programs defined by a grammar. As we do here, they
explain the inductive leap of the CP transition as a result of the
tension between program complexity and fit to the available data.
Crucially, whereas Piantadosi et al.~place a distribution over
programs using a probabilistic context-free grammar, our model is
learning a complete grammar, one that can accommodate many different
concepts and relations and that can be seen as a probabilistic and
declarative knowledge base.

Another difference is that our simulations require a pedagogical
emphasis on critical evidence -- the decade transitions -- to master
the count sequence robustly, suggesting that pedagogy may play an
important role in facilitating these kinds of inductive leaps.
Focusing on concept acquisition in slightly older children allows us
to explore the relationship between computational level considerations
driving inductive reasoning and the pedagogical factors enabling it in
practice.

An important goal for future work is to apply our model to learning
systems of number concepts in other languages besides English.  In
preliminary work we have applied our model to learning the Chinese
number system, shown that it can learn the adult system with relative
ease, and also that it can explain why Chinese children generally make
different patterns of mistakes than English-speaking children -- in
particular, why they are much less likely to invent number words like
``twenty eleven'' even though Chinese uses the same words to refer to
both decade and ones values (e.g., ``twenty one'' is ``two ten one'')~\citep{miller1987counting}.

Another important future step for this research will be to relate our
model to those, like Piantadosi et al. (\citeyear{PianGoodTen2012})
for counting and \citet{dehaene2011number} for the approximate magnitude system,
that attempt to explain how abstract number knowledge becomes grounded
in the perceptual and procedural primitives through which children
learn about the world. The model we presented here does not attempt to
explain how children come to understand that number words refer to
cardinalities, though this is crucial to understanding number.

That said, we see no fundamental incompatibility between the model
presented here and extensions to include approximate magnitude, object
tracking, set manipulation, more complex morphology (e.g. the
meaning of \emph{-illion} or \emph{-teen}), or different counting
strategies (e.g. as used in Turkish, French, or Mandarin) as
would be needed for a more comprehensive model of number learning. In
fact, a key next step for us is to model the link between the
relatively small set of named numbers (as modeled here) and systems
like Arabic or tally notation in which a truly infinite system is much
more easily expressed. We see our work here as a first demonstration
of LPN's suitability for capturing a broad range of concepts in number
and other semantic domains including space, kinship, and natural
kinds. Whether these more general models are best approached by
working strictly within the LPN formalism or by using it as one module
within a more complex framework is an open question. Certainly, the
human mind is more powerful than an RCG and is at least
Turing-complete. RCGs provide a tractable way, however, to explore a
restricted subclass of problems. The strategies and solutions we
discover here are also available in Turing-complete systems, and are
in fact implemented in one (PRISM Prolog), so our findings easily
generalize to more expressive grammars.

More broadly, we see this paper as growing out of the hypothesis that
much of human learning, including the explosion of knowledge during
development, can be understood as inducing, from sparse and noisy data,
a library of bits of conceptual knowledge, written in something like a
programming language of thought. This vision of the
\emph{child-as-hacker} draws on and extends the notion of the
\emph{child-as-scientist} \citep{gopnik1996scientist}; not only are
children forming theories about the world, but they are simultaneously
developing the very conceptual language they use to formulate those
theories.

\section{Acknowledgments}

The authors benefited significantly from conversations with Leon
Bergen, Timothy O'Donnell and Steven Piantadosi. This material is
based upon work supported by the Center for Minds, Brains and Machines
(CBMM), funded by NSF STC award CCF-1231216, an NSF Graduate Research
Fellowship, and the Eugene Stark Graduate Fellowship.

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}
\bibliography{cogsci}

\end{document}
